{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from smt.surrogate_models import KRG, RBF\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import keras\n",
    "from keras import layers\n",
    "import keras_tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Monte_Carlo_10000/training_data_bending.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['outer_wall_thickness', 'inside_wall_side_thickness', 'inside_wall_middle_thickness', 'height', 'width', 'sigma0', 'youngs']].values\n",
    "Y = df[['mean_force']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.Input(shape=(X_train.shape[1],)))\n",
    "    #model.add(layers.Dropout(rate=0.2))\n",
    "    #activation = hp.Choice('activation', values=['relu', 'tanh', 'sigmoid'])\n",
    "    units = hp.Int('units', min_value=16, max_value=128, step=16)\n",
    "    num_layers = hp.Int('num_layers', 1, 10)\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    for i in range(num_layers):\n",
    "        model.add(layers.Dense(\n",
    "            units=units, \n",
    "            activation='relu',\n",
    "            ))\n",
    "        #model.add(layers.Dropout(rate=0.5))\n",
    "    model.add(layers.Dense(\n",
    "        units=1, \n",
    "        activation='relu'))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.legacy.Adam(learning_rate),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mae']\n",
    "        )\n",
    "    return model\n",
    "\n",
    "hp = keras_tuner.HyperParameters()\n",
    "build_model(hp)\n",
    "\n",
    "tuner = keras_tuner.GridSearch(\n",
    "    hypermodel=build_model,\n",
    "    objective='val_mae',\n",
    "    executions_per_trial=5,\n",
    "    directory='keras_tuner',\n",
    "    project_name='test_15'\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2, \n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the hyperparameters and performance\n",
    "results = []\n",
    "for trial in tuner.oracle.get_best_trials(num_trials=1000):\n",
    "    result = {\n",
    "        'units': trial.hyperparameters.get(f'units'),\n",
    "        'num_layers': trial.hyperparameters.get('num_layers'),\n",
    "        'learning_rate': trial.hyperparameters.get('learning_rate'),\n",
    "        'mse': trial.score\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "# Create DataFrame\n",
    "hyperparameters_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = tuner.get_best_models(num_models=5)\n",
    "best_model = models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_df.head()\n",
    "#hyperparameters_df.to_csv('hyperparameters_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import LogFormatterMathtext\n",
    "\n",
    "# Load the hyperparameters DataFrame\n",
    "#hyperparameters_1_df = pd.read_csv('hyperparameters_1.csv')\n",
    "#hyperparameters_2_df = pd.read_csv('hyperparameters_2.csv')\n",
    "#combined_df = pd.concat([hyperparameters_1_df, hyperparameters_2_df])\n",
    "\n",
    "# Set learning rates to create one plot per learning rate\n",
    "learning_rates = hyperparameters_df['learning_rate'].unique()\n",
    "learning_rates = np.sort(learning_rates)\n",
    "print(learning_rates)\n",
    "\n",
    "# Determine the global minimum and maximum MSE values\n",
    "min_mse = hyperparameters_df['mse'].min()\n",
    "max_mse = hyperparameters_df['mse'].max()\n",
    "\n",
    "# Adjust general plot settings\n",
    "plt.rc('font', size=22)\n",
    "\n",
    "# Loop through each learning rate and create a separate figure\n",
    "for lr in learning_rates:\n",
    "    # Filter the dataset for each learning rate\n",
    "    subset = hyperparameters_df[hyperparameters_df['learning_rate'] == lr]\n",
    "    \n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(8, 6), dpi=300)\n",
    "    scatter = plt.scatter(\n",
    "        x=subset['units'], \n",
    "        y=subset['num_layers'], \n",
    "        c=subset['mse'],  # Use the original MSE values\n",
    "        cmap='coolwarm', \n",
    "        norm=LogNorm(vmin=min_mse, vmax=max_mse),  # Apply consistent logarithmic normalization\n",
    "        s=500, \n",
    "        edgecolor='none',  # Dots without edges\n",
    "        marker='o'  # Use dots as markers\n",
    "    )\n",
    "\n",
    "    # Set up color bar with scientific notation\n",
    "    colorbar = plt.colorbar(scatter, label='Validation MAE')\n",
    "    formatter = LogFormatterMathtext(base=10)  # Use scientific notation for ticks\n",
    "    colorbar.ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "    plt.xlabel('Units')\n",
    "    plt.ylabel('Number of Layers')\n",
    "    \n",
    "    # Save or show each plot separately without a title\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
